  # LLM and Generative AI Landscape

### Large Language Models (LLMs)
- A type of AI designed to process and generate human-like text.
- They are trained on massive datasets of text and code, allowing them to understand, summarize, translate, and generate new.
- Parameters are values in a model that are updated during the training of the model.
- Large language models have millions and billions of such parameters and are trained on enormous amounts of data.
- **GPT-3** (Generative Pre-trained Transformer 3) by OpenAI
- **LaMDA** (Language Model for Dialogue Applications) by Google
- **BERT** (Bidirectional Encoder Representations from Transformers) by Google

### Generative AI
- A broader category encompassing AI models capable of creating new content, such as images, music, and text.
- LLMs are a subset of GenAI, but the field also includes other models like:
- **GANs** (Generative Adversarial Networks): These models use two neural networks (a generator and a discriminator) to create realistic data.
- **VAEs** (Variational Autoencoders): These models use a probabilistic approach to generate new data.

### The Intersection of LLM and Generative AI
- LLMs can be used to generate human-quality text, which can then be used to train other generative models.
- For example, a GAN can be trained on a text dataset generated by an LLM to create realistic images of people.

### Applications of LLM and Generative AI
1. **Content Creation:** Generating articles, scripts, code, and other forms of content.
2. **Customer Service:** Providing personalized customer support through chatbots and virtual assistants.
3. **Education:** Creating personalized learning experiences and tutoring.
4. **Research:** Analyzing large datasets and generating new insights.
5. **Creative Arts:** Creating music, art, and other forms of creative expression.

# Embeddings, Attention Mechanism, and Transformer Architecture

### Embeddings
- A technique used to represent complex data, such as words or images, as numerical vectors.
- These vectors capture the semantic or contextual relationships between different data points.
- Word embeddings represent the meaning and context of a word, allowing the model to understand how words relate to each other.

### Attention Mechanism
- A technique that allows a model to focus on specific parts of an input sequence when processing it.
- This is particularly useful for tasks like machine translation or text summarization, where the model needs to weigh the importance of different parts of the input sequence.
- In essence, the attention mechanism calculates a "weight" for each element of the input sequence.
- Elements with higher weights are given more attention by the model.
- This allows the model to dynamically focus on the most relevant parts of the input, improving its performance.

### Transformer Architecture
- A neural network architecture that has revolutionized the field of NLP. Transformers are made up of encoders and decoders
- It is based on the attention mechanism and is designed to process sequential data, such as text.
- **Encoder:** Processes the input sequence and converts/generates a sequence of numerical vectors.
- **Decoder:** Generates the output sequence, using the numerical representations from the encoder and the attention mechanism.
- **Self-attention:** Relate different parts of the input sequence to each other, capturing dependencies and relationships.
- **Positional encoding:** Encode the positional information of each element in the input sequence.
- The Transformer architecture does not rely on recurrent connections.

### Advantage of Transformer Architecture over traditional RNNs

- **Parallel processing:** The Transformer can process the entire input sequence in parallel, making it more efficient than RNNs.
- **Long-range dependencies:** The attention mechanism allows the Transformer to capture long-range dependencies in the input sequence.
- **Performance:** A variety of NLP tasks, such as machine translation, text summarization, and question answering.

### Transfer Learning
Transfer learning is made up of two components:
1. **Pretraining:**
2. **Finetuning:**

### Pre-Training | How are LLM Models trained?
- Initially, the language model has random weights, and at this point, the model does not know the language.
- But if you train the model and pass a large corpus of data, it adjusts these weights as part of the training process.
- The pre-training stage is very resource-heavy, so you need lots of variety of data (books, articles, websites)
- You also need a lot of computing (processing capability) Nvidia GPUs, and Google TPUs.
- They run a lot of language model training tasks in parallel.
- Pre-training is a very compute-heavy and expensive process including a lot of hardware and data.

### Finetuning
- There are two essential components: Supervised fine-tuning and Reinforcement Learning from Human Feedback (RLHF)
- **Supervised fine-tuning:** They would pass both the prompt and the expected output from the label of each prompt to the LLM and train it.
- After some training for a given prompt, the LLM model gets better at producing output that resembles what a label would have written.
- **RLHF:** Model is trained with multiple ratings and feedback. 
